{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resize_standardize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct4ONRRWX9L9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNZ-uW6VTDpY"
      },
      "source": [
        "import wave\n",
        "import os\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AvTP8ecTFFP"
      },
      "source": [
        "Path_to_train = \"train\\wav\"\n",
        "subfolders = os.listdir(Path_to_train)\n",
        "data = []\n",
        "for s in subfolders:\n",
        "    files = os.listdir(Path_to_train + \"/\" +s)\n",
        "    data.extend([Path_to_train + \"/\" + s+ \"/\" + f for f in files])\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeZyvnnJTFHf"
      },
      "source": [
        "#read text from every transcription audio\n",
        "def read_text( text_path):\n",
        "    text = []\n",
        "    with open(text_path) as fp:\n",
        "        line = fp.readline()\n",
        "        while line:\n",
        "        # TODO: fix spaces in in amharic text\n",
        "            text.append(line)\n",
        "            line = fp.readline()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykuyj212TFK-"
      },
      "source": [
        "#extract the transcription and the label \n",
        "label=[]\n",
        "transcriptions = []\n",
        "for t in text:\n",
        "    sp = t.split(\"\\t\")\n",
        "    sp = sp.strip(\"\\n\")\n",
        "    if len(sp) > 1:\n",
        "        label.append(sp[0])\n",
        "        transcriptions.append(sp[1])\n",
        "transcriptions[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8mqLRCHTY8L"
      },
      "source": [
        "#get audio path , every path must corespond to transcription , get the transprion in the doc and append to audio path \n",
        "audio_path=[0]*len(transcriptions)\n",
        "for d in data:\n",
        "    _d = d.strip(\".wav\")\n",
        "    sp = _d.split(\"/\")[2]\n",
        "    index = label.index(sp)\n",
        "    audio_path[index] = d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atBdLgb2TY_i"
      },
      "source": [
        "#calculate duration \n",
        "duration_of_recordings=[]\n",
        "for d in audio_path:\n",
        "    audio, fs = librosa.load(d, sr=None)\n",
        "    duration_of_recordings.append(float(len(audio)/fs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuUupPpyTZCP"
      },
      "source": [
        "import pandas as pd \n",
        "data=pd.DataFrame({'key': audio_path,'text': transcriptions, 'duration':duration_of_recordings})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfqPBqisTsd0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mNnSAu0rvhl"
      },
      "source": [
        "# Resize and standardize\n",
        "We resize all the audio samples to have the same length by either extending its duration by padding it with silence, or by truncating it.\n",
        "We use the right padding method\n",
        "\n",
        "Standardize the sample rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNbZwxa3TFOX"
      },
      "source": [
        "# used to load audio file\n",
        "#specifying sample rate will resize all the files i.e Audio will be automatically resampled to the given rate\n",
        "class Loader:\n",
        "  def __init__(self, sample_rate,duration,mono):\n",
        "    self.sample_rate=sample_rate\n",
        "    self.duration=duration\n",
        "    self.mono=mono\n",
        "    self.channel = 2\n",
        "\n",
        "  def load_signal(self, filepath):\n",
        "    signal=librosa.load(filepath,\n",
        "                        sr=None,\n",
        "                        duration=self.duration,\n",
        "                        mono=self.mono)[0]    #librosa returns 2D array (signal,sample_rate) pick the signal here\n",
        "    return signal\n",
        "\n",
        "  def load_sample_rate(self,filepath):\n",
        "    self.sample_rate=librosa.load(filepath,\n",
        "                        sr =None,\n",
        "                        duration=self.duration,\n",
        "                        mono=self.mono)[1]    #librosa returns 2D array (signal,sample_rate) pick the sample rate here\n",
        "    return self.sample_rate\n",
        "\n",
        "  # def load(self,filepath):     #get both signal and sample rate in one\n",
        "  #   aud=librosa.load(filepath,\n",
        "  #                       sr=None,\n",
        "  #                       duration=self.duration,\n",
        "  #                       mono=self.mono)\n",
        "  #   return aud\n",
        "\n",
        "  def load(self,filepath):\n",
        "    sig, sr = torchaudio.load(filepath)\n",
        "    aud = sig, sr\n",
        "    return aud\n",
        "\n",
        "  #before using this function kindly change your file paths for it to work\n",
        "\n",
        "\n",
        "  def make_stereo(self,aud):\n",
        "      #this function converts mono audio channels into stereo channels \n",
        "  #     logging.info(\" ============ Conerting audio sample from mono to stereo ================= \")\n",
        "      print(\"======= Mono to stereo audio conversion\")\n",
        "      ifile = aud\n",
        "      #log the info on adio files\n",
        "  #     logging.info(ifile.getparams())\n",
        "      print (ifile.getparams())\n",
        "      # (1, 2, 44100, 2013900, 'NONE', 'not compressed')\n",
        "      (nchannels, sampwidth, framerate, nframes, comptype, compname) = ifile.getparams()\n",
        "      assert (comptype == 'NONE')  # Compressed not supported yet\n",
        "      array_type = {1:'B', 2: 'h', 4: 'l'}[sampwidth]\n",
        "      print(\" ======= Calculting left channel type =====\")\n",
        "      left_channel = array.array(array_type, ifile.readframes(nframes))[::nchannels]\n",
        "      ifile.close()\n",
        "\n",
        "      #convert the number of channels to 2\n",
        "      print(\"====== converting channels ======= \")\n",
        "      stereo = 2 * left_channel\n",
        "      stereo[0::2] = stereo[1::2] = left_channel\n",
        "      #overwrite the wav file making it a stereo file\n",
        "      print(\"====== overwriting wav file ======= \")\n",
        "      ofile = wave.open(audio_path, 'w')\n",
        "      ofile.setparams((2, sampwidth, framerate, nframes, comptype, compname))\n",
        "      ofile.writeframes(stereo.tostring())\n",
        "      ofile.close()\n",
        "\n",
        "  def resample(self,aud):                    #standardize sample rate\n",
        "    sig, sr = aud\n",
        "    \n",
        "    if (sr == self.sample_rate):\n",
        "      # Nothing to do\n",
        "      return aud[0]\n",
        "\n",
        "    num_channels = sig.shape[0]\n",
        "    # Resample first channel\n",
        "    resig = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[:1,:])\n",
        "    if (num_channels > 1):\n",
        "      # Resample the second channel and merge both channels\n",
        "      retwo = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[1:,:])\n",
        "      resig = torch.cat([resig, retwo])\n",
        "      aud = resig, self.sample_rate\n",
        "    return aud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikidMk81TFQt"
      },
      "source": [
        "#resizes audios to have same length\n",
        "class Padder:\n",
        "  def __init__(self,mode=\"constant\"):\n",
        "    self.mode=mode\n",
        "  def left_pad(self,array,num_missing_items):\n",
        "    padded_array=np.pad(array,\n",
        "                        (num_missing_items, 0),\n",
        "                        mode=self.mode)\n",
        "    return padded_array\n",
        "  def right_pad(self,array,num_missing_items):\n",
        "    padded_array=np.pad(array,\n",
        "                        (0,num_missing_items),\n",
        "                        mode=self.mode)\n",
        "    return padded_array\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkLUE2j5TFS0"
      },
      "source": [
        "class PreprocessingPipeline:\n",
        "  '''Processes audio files in a directory by applying the following steps\n",
        "    1. Load the data, convert to stereo and resample sampling rate\n",
        "    2. Pad the audio\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.padder=None\n",
        "    self._loader=None\n",
        "   \n",
        "\n",
        "  def process(self,audio_files_directory):\n",
        "    for root, directories, files in os.walk(audio_files_directory):\n",
        "        for filename in files:\n",
        "            filepath = os.path.join(root, filename)\n",
        "            self._process_file(filepath)\n",
        "            print(f\"Processed file {filepath}\")\n",
        "    \n",
        "  def _process_file(self,filepath):\n",
        "    signal=self.loader.load(filepath)\n",
        "    signal = self.loader.make_stereo(signal)\n",
        "    signal = self.loader.resample(signal)[0]\n",
        "    if self._is_padding_necessary(signal):\n",
        "      signal=self._apply_padding(signal)\n",
        "    \n",
        "\n",
        "  def _is_padding_necessary(self,signal):\n",
        "    self.num_expected_samples=int(loader.sample_rate*loader.duration)\n",
        "    if len(signal) < self.num_expected_samples:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _apply_padding(self,signal):\n",
        "    num_missing_samples=self.num_expected_samples - len(signal)\n",
        "    padded_signal = self.padder.right_pad(signal, num_missing_samples)\n",
        "    return padded_signal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeWtzfg9TFVS"
      },
      "source": [
        "DURATION=0.74\n",
        "SAMPLE_RATE=22050\n",
        "MONO=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObprKDYvUPOb"
      },
      "source": [
        "loader=Loader(SAMPLE_RATE, DURATION, MONO)\n",
        "padder=Padder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBtGze_5UPRG"
      },
      "source": [
        "preprocessing_pipeline=PreprocessingPipeline()\n",
        "preprocessing_pipeline.loader=loader\n",
        "preprocessing_pipeline.padder=padder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf0oX_sfUPT-"
      },
      "source": [
        "preprocessing_pipeline.process(Path_to_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef5TTLS0UPm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}